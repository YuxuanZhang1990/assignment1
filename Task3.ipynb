{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('先生早上好！我是1号客服，您想咨询餐饮业务吗？', 4.0063717610968116e-19)\n"
     ]
    }
   ],
   "source": [
    "def generate_best(n):\n",
    "    L=generate_n(n)\n",
    "    r=sorted(L, key=lambda x: x[1], reverse=True)\n",
    "    print(r[0])\n",
    "    \n",
    "import random\n",
    "\n",
    "def get_generation_by_gram(grammar_str: str, target, stmt_split='=', or_split='|'):\n",
    "    rules = dict()  # key is the @statement, value is @expression\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line: continue\n",
    "        # skip the empty line\n",
    "        #  print(line)\n",
    "        stmt, expr = line.split(stmt_split)\n",
    "        rules[stmt.strip()] = expr.split(or_split)\n",
    "    generated = generate(rules, target=target)\n",
    "    return generated\n",
    "\n",
    "\n",
    "customer_service_grammar = \"\"\"\n",
    "    cs => title say_hello 我是 Number 号客服，您想 verb content end\n",
    "    title => 女士 | 先生 | 小朋友\n",
    "    say_hello => 早上好！ | 下午好！ |晚上好！\n",
    "    Number => 1 | 2 |3 |5\n",
    "    verb =>  了解 | 打印 | 咨询\n",
    "    content =>  订房业务 | 餐饮业务 | 娱乐活动 |优惠折扣\n",
    "    end => 吗？\"\"\"\n",
    "\n",
    "\n",
    "def generate(grammar_rule, target):\n",
    "    if target in grammar_rule: \n",
    "        candidates = grammar_rule[target]  \n",
    "        candidate = random.choice(candidates)  \n",
    "        return ''.join(generate(grammar_rule, target=c.strip()) for c in candidate.split())\n",
    "    else:\n",
    "        return target\n",
    "\n",
    "\n",
    "get_generation_by_gram(customer_service_grammar, target='cs', stmt_split='=>')\n",
    "\n",
    "\n",
    "import jieba\n",
    "corpus = 'article_9k.txt'\n",
    "FILE = open(corpus).read()\n",
    "max_length = 1000000 \n",
    "sub_file = FILE[:max_length]\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "TOKENS = cut(sub_file)\n",
    "\n",
    "from collections import Counter\n",
    "words_count = Counter(TOKENS)\n",
    "_2_gram_words = [\n",
    "    TOKENS[i] + TOKENS[i+1] for i in range(len(TOKENS)-1)\n",
    "]\n",
    "_2_gram_words[:10]\n",
    "_2_gram_word_counts = Counter(_2_gram_words)\n",
    "words_count.most_common()[-1][-1]\n",
    "\n",
    "def get_1_gram_count(word):\n",
    "    if word in words_count: return words_count[word]\n",
    "    else:\n",
    "        return words_count.most_common()[-1][-1]\n",
    "def get_2_gram_count(word):\n",
    "    if word in _2_gram_word_counts: return _2_gram_word_counts[word]\n",
    "    else:\n",
    "        return _2_gram_word_counts.most_common()[-1][-1]\n",
    "def get_gram_count(word, wc):\n",
    "    if word in wc: return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1]\n",
    "    \n",
    "def two_gram_model(sentence):\n",
    "    # 2-gram langauge model\n",
    "    tokens = cut(sentence)    \n",
    "    probability = 1 \n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]        \n",
    "        _two_gram_c = get_gram_count(word+next_word, _2_gram_word_counts)\n",
    "        _one_gram_c = get_gram_count(next_word, words_count)\n",
    "        pro =  _two_gram_c / _one_gram_c       \n",
    "        probability *= pro \n",
    "    return probability  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_n(n):\n",
    "    L=[]\n",
    "    for i in range(n):\n",
    "      content=get_generation_by_gram(customer_service_grammar, target='cs', stmt_split='=>')\n",
    "      pro=two_gram_model(content)\n",
    "      L.append((content,pro))\n",
    "    return L\n",
    "\n",
    "generate_best(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
